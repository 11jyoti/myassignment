{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527f6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\u001b' Explain the concept of forward propagation in a neural network'\n",
    "Ans---\n",
    "Concept of Forward Propagation in a Neural Network\n",
    "Forward propagation is the process through which input data passes through a neural network, layer by layer, to generate an output prediction. It is the first step in training and inference before backpropagation is applied for learning.\n",
    "\n",
    "1. Overview of Forward Propagation\n",
    "Takes an input (e.g., an image or numerical data).\n",
    "Applies weights, biases, and activation functions in each layer.\n",
    "Produces an output (e.g., classification label or regression value).\n",
    "The key steps in forward propagation include:\n",
    "\n",
    "Input Layer Processing\n",
    "Linear Transformation (Weighted Sum & Bias Addition)\n",
    "Activation Function Application\n",
    "Propagation Through Hidden Layers\n",
    "Output Layer Computation\n",
    "\n",
    "\n",
    "\u0013' What is the purpose of the activation function in forward propagation\n",
    "Ans--\n",
    "Purpose of the Activation Function in Forward Propagation\n",
    "In forward propagation, an activation function introduces non-linearity into a neural network, enabling it to learn complex patterns and relationships in data. Without activation functions, a neural network would behave like a simple linear regression model, no matter how many layers it has.\n",
    "\n",
    "\n",
    "\n",
    "\u000e' Describe the steps involved in the backward propagation (backpropagation) algorithm'\n",
    "ans--\n",
    "1. Forward Propagation (Compute Output)\n",
    "Input \n",
    "ùëã\n",
    "X passes through the network layers.\n",
    "We compute weighted sums and apply activation functions to obtain the predicted output \n",
    "\n",
    "2. Compute the Loss (Error Calculation)\n",
    "A loss function measures the difference between predicted output \n",
    "\n",
    "3. Compute Gradients (Error Propagation)\n",
    "The goal is to find how much each weight and bias contributes to the total error.\n",
    "Using the chain rule of differentiation, we calculate gradients for each layer from the output layer back to the input.\n",
    "Gradient Calculation for Output Layer\n",
    "Compute the gradient of the loss \n",
    "\n",
    "This process propagates errors backward, updating each layer‚Äôs parameters.\n",
    "4. Update Weights and Biases Using Gradient Descent\n",
    "Weights and biases are updated using gradient descent:\n",
    "\n",
    "5. Repeat Until Convergence\n",
    "Steps 1 to 4 are repeated for multiple iterations (epochs) until the loss reaches a minimum, improving model accuracy.\n",
    "\n",
    "\n",
    "' What is the purpose of the chain rule in backpropagation\n",
    "Ans--\n",
    "Purpose of the Chain Rule in Backpropagation\n",
    "The chain rule is a fundamental concept in calculus that allows us to compute the derivative of a function that is composed of multiple functions. In backpropagation, the chain rule is used to efficiently compute gradients for updating the weights and biases in a multi-layer neural network.\n",
    "\n",
    "Why is the Chain Rule Needed in Backpropagation?\n",
    "Backpropagation works by computing how much each weight in the network contributes to the total error. Since a neural network consists of multiple layers, the output error depends on all the layers before it.\n",
    "\n",
    "The chain rule helps propagate this error backward through the layers by computing gradients layer by layer.\n",
    "This allows us to efficiently calculate the derivative of the loss function with respect to each weight and bias.\n",
    "\n",
    "\n",
    "' Implement the forward propagation process for a simple neural network with one hidden layer using\n",
    "NumPy\n",
    "Ans--\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Input data (2 features, 1 sample)\n",
    "X = np.array([[0.5, 0.8]])  # Shape (1,2)\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)  # For reproducibility\n",
    "W1 = np.random.randn(2, 3)  # Weights for input to hidden layer (2x3)\n",
    "b1 = np.random.randn(1, 3)  # Bias for hidden layer (1x3)\n",
    "W2 = np.random.randn(3, 1)  # Weights for hidden to output layer (3x1)\n",
    "b2 = np.random.randn(1, 1)  # Bias for output layer (1x1)\n",
    "\n",
    "# Forward propagation\n",
    "Z1 = np.dot(X, W1) + b1  # Compute weighted sum for hidden layer\n",
    "A1 = relu(Z1)            # Apply ReLU activation\n",
    "\n",
    "Z2 = np.dot(A1, W2) + b2  # Compute weighted sum for output layer\n",
    "A2 = sigmoid(Z2)         # Apply Sigmoid activation\n",
    "\n",
    "# Output result\n",
    "print(\"Output of the network:\", A2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f0a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
